{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import time\n",
        "from jax import vmap, jit"
      ],
      "metadata": {
        "id": "LrsI8vEyizSf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UuWtvN6hmjP",
        "outputId": "64055bf0-2cd7-4144-e2dc-73af61b8450b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150,), (150,), 2231275)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "ar1 = np.arange(150)\n",
        "ar2 = np.arange(100,250)\n",
        "result = np.dot(ar1,ar2)\n",
        "ar1.shape, ar2.shape, result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = jnp.stack([jnp.arange(150) for i in range(100)]) # 100 is the batch dimension!!\n",
        "array2 = jnp.stack([jnp.arange(100, 250) for i in range(100)])\n",
        "array1.shape, array2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXhKOArgjFyP",
        "outputId": "be03ea4e-da13-4e24-d6ba-eed4e0da2af0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 150), (100, 150))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive way\n",
        "start = time.time()\n",
        "\n",
        "output = []\n",
        "for i in range(100):\n",
        "    output.append(jnp.dot(array1[i], array2[i]))\n",
        "\n",
        "output = jnp.stack(output)\n",
        "print(output)\n",
        "print('Output shape: ', output.shape)\n",
        "time_taken = time.time() - start\n",
        "print('Time take in secs: ', time_taken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VJsmO2DcVG2",
        "outputId": "3f3844ed-f294-419e-eabc-4718474197cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275]\n",
            "Output shape:  (100,)\n",
            "Time take in secs:  0.8910415172576904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vmap(jnp.dot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wgdg-lrWdByF",
        "outputId": "c1dcfcfe-4a85-497f-835e-7d2619fe64da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function jax._src.numpy.lax_numpy.dot(a: 'ArrayLike', b: 'ArrayLike', *, precision: 'PrecisionLike' = None, preferred_element_type: 'DTypeLike | None' = None) -> 'Array'>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>jax._src.numpy.lax_numpy.dot</b><br/>def dot(a: ArrayLike, b: ArrayLike, *, precision: PrecisionLike=None, preferred_element_type: DTypeLike | None=None) -&gt; Array</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py</a>Vectorized version of dot. Takes similar arguments as dot but with additional array axes over which dot is mapped.\n",
              "\n",
              "Original documentation:\n",
              "\n",
              "Dot product of two arrays. Specifically,\n",
              "\n",
              "LAX-backend implementation of :func:`numpy.dot`.\n",
              "\n",
              "In addition to the original NumPy arguments listed below, also supports\n",
              "``precision`` for extra control over matrix-multiplication precision\n",
              "on supported devices. ``precision`` may be set to ``None``, which means\n",
              "default precision for the backend, a :class:`~jax.lax.Precision` enum value\n",
              "(``Precision.DEFAULT``, ``Precision.HIGH`` or ``Precision.HIGHEST``) or a tuple\n",
              "of two :class:`~jax.lax.Precision` enums indicating separate precision for each argument.\n",
              "\n",
              "*Original docstring below.*\n",
              "\n",
              "- If both `a` and `b` are 1-D arrays, it is inner product of vectors\n",
              "  (without complex conjugation).\n",
              "\n",
              "- If both `a` and `b` are 2-D arrays, it is matrix multiplication,\n",
              "  but using :func:`matmul` or ``a @ b`` is preferred.\n",
              "\n",
              "- If either `a` or `b` is 0-D (scalar), it is equivalent to\n",
              "  :func:`multiply` and using ``numpy.multiply(a, b)`` or ``a * b`` is\n",
              "  preferred.\n",
              "\n",
              "- If `a` is an N-D array and `b` is a 1-D array, it is a sum product over\n",
              "  the last axis of `a` and `b`.\n",
              "\n",
              "- If `a` is an N-D array and `b` is an M-D array (where ``M&gt;=2``), it is a\n",
              "  sum product over the last axis of `a` and the second-to-last axis of\n",
              "  `b`::\n",
              "\n",
              "    dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
              "\n",
              "It uses an optimized BLAS library when possible (see `numpy.linalg`).\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "a : array_like\n",
              "    First argument.\n",
              "b : array_like\n",
              "    Second argument.\n",
              "preferred_element_type : dtype, optional\n",
              "    If specified, accumulate results and return a result of the given data type.\n",
              "    If not specified, the accumulation dtype is determined from the type promotion\n",
              "    rules of the input array dtypes.\n",
              "\n",
              "Returns\n",
              "-------\n",
              "output : ndarray\n",
              "    Returns the dot product of `a` and `b`.  If `a` and `b` are both\n",
              "    scalars or both 1-D arrays then a scalar is returned; otherwise\n",
              "    an array is returned.\n",
              "    If `out` is given, then it is returned.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 3325);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "output = vmap(jnp.dot)(array1, array2)\n",
        "print(output)\n",
        "print('Output shape: ', output.shape)\n",
        "time_taken = time.time() - start\n",
        "print('Time take in secs: ', time_taken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAUNO0AzdyNf",
        "outputId": "bb86484d-7dc7-4e02-9fa4-605f0411b8fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275]\n",
            "Output shape:  (100,)\n",
            "Time take in secs:  0.10920429229736328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ouput = vmap(jnp.dot, in_axes = (0, 0))(array1, array2) # 'in_axes' explicitly specifies the batch dimensions\n",
        "print(output)\n",
        "print('Output shape: ', output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow4eOMGqeLlF",
        "outputId": "4393a563-8e97-4d44-a2f3-e0735654ccc2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275]\n",
            "Output shape:  (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array1 = jnp.arange(150)\n",
        "array1.shape, array2.shape # array1 is a single array and array2 has a batch of 100 arrays"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx1531m3hW3v",
        "outputId": "139bca82-a6ab-4c3a-ff00-d9f215a15514"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150,), (100, 150))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = vmap(jnp.dot, in_axes = (None, 0))(array1, array2) # The first array is a vector, it doesn't have a batch, therefore 'None'\n",
        "print(output)\n",
        "print('Output shape: ', output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZWU_9iahela",
        "outputId": "198ae0d8-174e-443b-f20f-dd973c6e98e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275 2231275\n",
            " 2231275]\n",
            "Output shape:  (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "W = jax.random.normal(key, (64, 100), dtype=jnp.float32) # input dimension=100, 64 neurons (output size of the linear layer)\n",
        "batch_x = jax.random.normal(key, (16, 100), dtype=jnp.float32) # batch dimension=16, input dimension=100\n",
        "\n",
        "W.shape, batch_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y355CTmig6t",
        "outputId": "841cacb3-db63-4802-ebdb-4130824ae91f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((64, 100), (16, 100))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard linear layer without bias and activation function for a single data point (not batch)\n",
        "def layer(x):\n",
        "    # (64, 100) . (100, ) -> (64, )\n",
        "    return jnp.dot(W, x)"
      ],
      "metadata": {
        "id": "YVwKVoy9PIaG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer(batch_x) # gives error, because the layer() accepts a single data point, not batch of data points"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "8YLy6H46QP97",
        "outputId": "370ef28e-70f7-478d-b542-ef4075f030c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "dot_general requires contracting dimensions to have the same shape, got (100,) and (16,).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8fd1b8e1888b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gives error, because the layer() accepts a single data point, not batch of data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-fe5acea93792>\u001b[0m in \u001b[0;36mlayer\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# (64, 100) . (100, ) -> (64, )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   3351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3352\u001b[0m       \u001b[0mcontract_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_ndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3353\u001b[0;31m     result = lax.dot_general(a, b, dimension_numbers=(contract_dims, batch_dims),\n\u001b[0m\u001b[1;32m   3354\u001b[0m                              precision=precision, preferred_element_type=preferred_element_type)\n\u001b[1;32m   3355\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_element_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_weak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   2658\u001b[0m     msg = (\"dot_general requires contracting dimensions to have the same \"\n\u001b[1;32m   2659\u001b[0m            \"shape, got {} and {}.\")\n\u001b[0;32m-> 2660\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_contracting_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_contracting_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2662\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_dot_general_shape_computation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (100,) and (16,)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(layer(batch_x[0]))\n",
        "layer(batch_x[0]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTdX72UJQS6P",
        "outputId": "11771f0e-3dd5-4c53-d291-b54caf76202b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -1.3889995  -20.139408   -15.25461     12.268584   -11.33385\n",
            "  22.630579     0.6938026  -13.827614    11.879179    -3.962661\n",
            "  18.831707   -14.518444   -10.260715   -12.685417     2.5124693\n",
            "  -4.255941    -1.3663092    6.9495125   -7.8258133   -8.293367\n",
            "  -6.7460346  -29.767746    -4.768341    14.712051    -1.9340603\n",
            "   6.222947    13.89996    -11.409643    -3.27421     -2.1721942\n",
            "  10.826935    -2.5647302   -0.46695042 -11.210756    -7.741742\n",
            " -22.293253     5.421151     1.3914757    3.3206859   -8.409931\n",
            "   2.869808     7.1217394    3.5472736   -4.937554    -1.475796\n",
            "  -4.0422435   -8.101667     0.17466402  -3.5307515   -8.768582\n",
            "  14.79269      0.30482996  20.986172    -0.58729076   6.27522\n",
            " -20.083494     5.8386555  -13.792967   -10.024259     3.3196595\n",
            "  15.8581       5.4580092   -6.9915285   27.747955  ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that this cannot be jitted, because we rely on the content of the input\n",
        "def naive_batched_layer(batch_x):\n",
        "    outputs = []\n",
        "    for row in batch_x:\n",
        "        outputs.append(layer(row))\n",
        "    return jnp.stack(outputs)"
      ],
      "metadata": {
        "id": "j2cEhay9Qpy7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Naive batching')\n",
        "\n",
        "%timeit naive_batched_layer(batch_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43nBiudpRj83",
        "outputId": "5d31c11d-2aaa-4128-8261-83d6622a1985"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive batching\n",
            "3.36 ms ± 66.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def manual_batched_layer(batch_x):\n",
        "    # (16, 100) . (100, 64) -> (16, 64)\n",
        "    return jnp.dot(batch_x, W.T)"
      ],
      "metadata": {
        "id": "tFTcECvJR5gJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Manual batching')\n",
        "\n",
        "%timeit manual_batched_layer(batch_x).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaS3GDkCST2R",
        "outputId": "7713d0b9-b826-4817-9e41-eba1029d906c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual batching\n",
            "145 µs ± 42.1 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def vmap_batched_layer(batch_x):\n",
        "    return vmap(layer)(batch_x)"
      ],
      "metadata": {
        "id": "ZeZk4ZcuSq-5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Auto-vectorized batching')\n",
        "\n",
        "%timeit vmap_batched_layer(batch_x).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g77wfsjES9jF",
        "outputId": "844578d6-506c-4404-9ab0-ab9e822175c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-vectorized batching\n",
            "106 µs ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_with_weights(W, x):\n",
        "    # (64, 100) . (100, ) -> (64, )\n",
        "    return jnp.dot(W, x)"
      ],
      "metadata": {
        "id": "Pn8vaKVMTaEe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def vmap_batched_layer_with_weights(W, batch_x):\n",
        "    return vmap(layer_with_weights, in_axes=(None, 0))(W, batch_x)"
      ],
      "metadata": {
        "id": "GNn4i1-wT7ue"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Auto-vectorized batching')\n",
        "\n",
        "%timeit vmap_batched_layer_with_weights(W, batch_x).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjj-9dcfUlDV",
        "outputId": "c656556d-57a6-4805-c8f7-fddac96addb5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-vectorized batching\n",
            "116 µs ± 21.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pBCiB9dDUu71"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}